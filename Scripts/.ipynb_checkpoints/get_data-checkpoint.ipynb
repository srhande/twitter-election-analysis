{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filename: get_data.pynb\n",
    "\n",
    "# Purpose:\n",
    "\n",
    "This script will download the tweets we need for our project and put them into a pandas DataFrame. There are several ways we can get the data we need for our project.\n",
    "\n",
    "As of right now, we use two methods to get the data we need: through using specific keywords regarding the 2020 Presidential Election and by scraping tweets with hashtags regarding the 2020 Presidential election and its candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading https://files.pythonhosted.org/packages/36/1b/2bd38043d22ade352fc3d3902cf30ce0e2f4bf285be3b304a2782a767aec/tweepy-3.8.0-py2.py3-none-any.whl\n",
      "Collecting requests-oauthlib>=0.7.0 (from tweepy)\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/e2/9fd03d55ffb70fe51f587f20bcf407a6927eb121de86928b34d162f0b1ac/requests_oauthlib-1.2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests>=2.11.1 in d:\\anaconda\\lib\\site-packages (from tweepy) (2.21.0)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in d:\\anaconda\\lib\\site-packages (from tweepy) (1.6.8)\n",
      "Requirement already satisfied: six>=1.10.0 in d:\\anaconda\\lib\\site-packages (from tweepy) (1.12.0)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->tweepy)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests>=2.11.1->tweepy) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from requests>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests>=2.11.1->tweepy) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests>=2.11.1->tweepy) (1.24.1)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-3.1.0 requests-oauthlib-1.2.0 tweepy-3.8.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install tweepy # Run this line only if you don't have tweepy installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import tweepy \n",
    "from tweepy import OAuthHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Tweets using Twitter API with `Tweepy`\n",
    "\n",
    "The below class is made using [this code](https://www.kaggle.com/amar09/sentiment-analysis-on-scrapped-tweets?source=post_page-----1804db3478ac----------------------) from Kaggle User [Amardeep Chauhan](https://www.kaggle.com/amar09). \n",
    "\n",
    "The class uses `tweepy` to access the Twitter API and fetch tweets relating to a specified keyword. \n",
    "\n",
    "The keywords we will use are:\n",
    "\n",
    "- 2020 Presidential Election\n",
    "- #2020Election\n",
    "- #2020PresidentialElection\n",
    "- #Election2020\n",
    "- #KnowThe2020Candidates\n",
    "- #POTUS2020\n",
    "- #2020America\n",
    "- TODO: Add names of all candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys and tokens\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "\n",
    "access_token = ''\n",
    "access_token_secret = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterClient(object):\n",
    "    \"\"\"\n",
    "    Initialization method. Creates a tweepy API object in order to use tweets.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            # Create OAuthHandler Object\n",
    "            auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "            \n",
    "            # Set access token and secret token\n",
    "            auth.set_access_token(access_token, access_token_secret)\n",
    "            \n",
    "            # Create tweepy API object to fetch tweets\n",
    "            self.api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)\n",
    "            \n",
    "        except tweepy.TweepError as e:\n",
    "            print(f'Error: Tweeter Authentication Failed - \\n{str(e)}')\n",
    "            \n",
    "    \"\"\"\n",
    "    Fetches tweets using a specified query. Stores the tweets in a list after\n",
    "    parsing them, which means extracting only the text and appending only unique tweets \n",
    "    to the resultant list.\n",
    "    \n",
    "    self: The TwitterClient object that will help us use the Twitter API.\n",
    "    query: The specified query to search for.\n",
    "    max_tweets: The maximum number of tweets in total to fetch. The default is 1000.\n",
    "    \n",
    "    Returns a pandas DataFrame of unique tweets relating to the keyword. The shape is not\n",
    "    necessarily going to have max_tweets entries due to retweets. \n",
    "    \"\"\"\n",
    "    def get_tweets(self, query, max_tweets = 1000):\n",
    "        tweets = []\n",
    "        since_Id = None\n",
    "        max_id = -1\n",
    "        tweet_count = 0\n",
    "        tweets_per_query = 100\n",
    "        \n",
    "        print('Fetching tweets for', query + '...')\n",
    "        \n",
    "        while tweet_count < max_tweets:\n",
    "            try:\n",
    "                # Try searching for tweets that have a max_id <= 0 i.e. older than -1.\n",
    "                if(max_id <= 0):\n",
    "                    if(not since_Id): # Get any tweets relating to the query\n",
    "                        new_tweets = self.api.search(q = query, count = tweets_per_query)\n",
    "                        \n",
    "                    else: # Get tweets more recent that since_Id\n",
    "                        new_tweets = self.api.search(q = query, count = tweets_per_query, since_id = since_Id)\n",
    "                else:\n",
    "                    if(not since_Id):\n",
    "                        new_tweets = self.api.search(q = query, count = tweets_per_query, max_id = str(max_id - 1))\n",
    "                        \n",
    "                    else:\n",
    "                        new_tweets = self.api.search(q = query, count = tweets_per_query, max_id = str(max_id - 1), \n",
    "                                                     since_id = since_Id)\n",
    "                \n",
    "                if not new_tweets:\n",
    "                    print('No more tweets found.')\n",
    "                    break\n",
    "                \n",
    "                # Start parsing the list of tweets\n",
    "                for tweet in new_tweets:\n",
    "                    parsed_tweet = {}\n",
    "                    parsed_tweet['tweets'] = tweet.text\n",
    "                    \n",
    "                    # Append parsed tweet to tweets list\n",
    "                    if tweet.retweet_count > 0: \n",
    "                        if parsed_tweet not in tweets: # If tweet has retweets, ensure that its appended only once\n",
    "                            tweets.append(parsed_tweet)\n",
    "                    else:\n",
    "                        tweets.append(parsed_tweet)\n",
    "                \n",
    "                tweet_count += len(new_tweets)\n",
    "                print('\\tDownloaded {0} tweets'.format(tweet_count))\n",
    "                max_id = new_tweets[-1].id # Prepare to get tweets that are older than the returned tweets \n",
    "                \n",
    "            except tweepy.TweepError as e: # Exit if there are any errors\n",
    "                print('Tweepy Error: ' + str(e))\n",
    "                break\n",
    "                \n",
    "        print('Finished!\\n')\n",
    "        \n",
    "        # Return DataFrame of tweets\n",
    "        return pd.DataFrame(tweets)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twitter_client = TwitterClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for each keyword \n",
    "keywords = ['2020 Presidential Election', '#2020Election', '#2020PresidentialElection', \n",
    "            '#Election2020', '#KnowThe2020Candidates', '#POTUS2020', '#2020America']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching tweets for 2020 Presidential Election...\n",
      "\tDownloaded 100 tweets\n",
      "\tDownloaded 200 tweets\n",
      "\tDownloaded 300 tweets\n",
      "\tDownloaded 400 tweets\n",
      "\tDownloaded 500 tweets\n",
      "\tDownloaded 600 tweets\n",
      "\tDownloaded 700 tweets\n",
      "\tDownloaded 800 tweets\n",
      "\tDownloaded 900 tweets\n",
      "\tDownloaded 1000 tweets\n",
      "\tDownloaded 1096 tweets\n",
      "\tDownloaded 1196 tweets\n",
      "\tDownloaded 1296 tweets\n",
      "\tDownloaded 1396 tweets\n",
      "\tDownloaded 1496 tweets\n",
      "\tDownloaded 1596 tweets\n",
      "\tDownloaded 1696 tweets\n",
      "\tDownloaded 1796 tweets\n",
      "\tDownloaded 1896 tweets\n",
      "\tDownloaded 1996 tweets\n",
      "\tDownloaded 2096 tweets\n",
      "\tDownloaded 2196 tweets\n",
      "\tDownloaded 2296 tweets\n",
      "\tDownloaded 2396 tweets\n",
      "\tDownloaded 2496 tweets\n",
      "\tDownloaded 2596 tweets\n",
      "\tDownloaded 2696 tweets\n",
      "\tDownloaded 2796 tweets\n",
      "\tDownloaded 2896 tweets\n",
      "\tDownloaded 2996 tweets\n",
      "\tDownloaded 3096 tweets\n",
      "\tDownloaded 3196 tweets\n",
      "\tDownloaded 3296 tweets\n",
      "\tDownloaded 3396 tweets\n",
      "\tDownloaded 3496 tweets\n",
      "\tDownloaded 3596 tweets\n",
      "\tDownloaded 3696 tweets\n",
      "\tDownloaded 3796 tweets\n",
      "\tDownloaded 3896 tweets\n",
      "\tDownloaded 3996 tweets\n",
      "\tDownloaded 4096 tweets\n",
      "\tDownloaded 4196 tweets\n",
      "\tDownloaded 4296 tweets\n",
      "\tDownloaded 4396 tweets\n",
      "\tDownloaded 4496 tweets\n",
      "\tDownloaded 4596 tweets\n",
      "\tDownloaded 4696 tweets\n",
      "\tDownloaded 4796 tweets\n",
      "\tDownloaded 4896 tweets\n",
      "\tDownloaded 4996 tweets\n",
      "\tDownloaded 5096 tweets\n",
      "Finished!\n",
      "\n",
      "tweets_df1 Shape - (462, 1)\n"
     ]
    }
   ],
   "source": [
    "tweets_df1 = twitter_client.get_tweets(keywords[0], max_tweets = 5000)\n",
    "print(f'tweets_df1 Shape - {tweets_df1.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching tweets for #2020Election...\n",
      "\tDownloaded 100 tweets\n",
      "\tDownloaded 200 tweets\n",
      "\tDownloaded 300 tweets\n",
      "\tDownloaded 400 tweets\n",
      "\tDownloaded 500 tweets\n",
      "\tDownloaded 600 tweets\n",
      "\tDownloaded 700 tweets\n",
      "\tDownloaded 800 tweets\n",
      "\tDownloaded 900 tweets\n",
      "\tDownloaded 1000 tweets\n",
      "\tDownloaded 1100 tweets\n",
      "\tDownloaded 1200 tweets\n",
      "\tDownloaded 1300 tweets\n",
      "\tDownloaded 1400 tweets\n",
      "\tDownloaded 1500 tweets\n",
      "\tDownloaded 1600 tweets\n",
      "\tDownloaded 1700 tweets\n",
      "\tDownloaded 1800 tweets\n",
      "\tDownloaded 1900 tweets\n",
      "\tDownloaded 2000 tweets\n",
      "\tDownloaded 2100 tweets\n",
      "\tDownloaded 2200 tweets\n",
      "\tDownloaded 2300 tweets\n",
      "\tDownloaded 2400 tweets\n",
      "\tDownloaded 2500 tweets\n",
      "\tDownloaded 2600 tweets\n",
      "\tDownloaded 2700 tweets\n",
      "\tDownloaded 2800 tweets\n",
      "\tDownloaded 2900 tweets\n",
      "\tDownloaded 3000 tweets\n",
      "\tDownloaded 3100 tweets\n",
      "\tDownloaded 3200 tweets\n",
      "\tDownloaded 3300 tweets\n",
      "\tDownloaded 3400 tweets\n",
      "\tDownloaded 3500 tweets\n",
      "\tDownloaded 3600 tweets\n",
      "\tDownloaded 3700 tweets\n",
      "\tDownloaded 3800 tweets\n",
      "\tDownloaded 3900 tweets\n",
      "\tDownloaded 4000 tweets\n",
      "\tDownloaded 4100 tweets\n",
      "\tDownloaded 4200 tweets\n",
      "\tDownloaded 4300 tweets\n",
      "\tDownloaded 4400 tweets\n",
      "\tDownloaded 4500 tweets\n",
      "\tDownloaded 4600 tweets\n",
      "\tDownloaded 4700 tweets\n",
      "\tDownloaded 4800 tweets\n",
      "\tDownloaded 4900 tweets\n",
      "\tDownloaded 5000 tweets\n",
      "Finished!\n",
      "\n",
      "tweets_df2 Shape - (2166, 1)\n"
     ]
    }
   ],
   "source": [
    "tweets_df2 = twitter_client.get_tweets(keywords[1], max_tweets = 5000)\n",
    "print(f'tweets_df2 Shape - {tweets_df2.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching tweets for #2020PresidentialElection...\n",
      "\tDownloaded 29 tweets\n",
      "\tDownloaded 129 tweets\n",
      "\tDownloaded 143 tweets\n",
      "No more tweets found.\n",
      "Finished!\n",
      "\n",
      "tweets_df3 Shape - (108, 1)\n"
     ]
    }
   ],
   "source": [
    "tweets_df3 = twitter_client.get_tweets(keywords[2], max_tweets = 5000)\n",
    "print(f'tweets_df3 Shape - {tweets_df3.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching tweets for #Election2020...\n",
      "\tDownloaded 100 tweets\n",
      "\tDownloaded 200 tweets\n",
      "\tDownloaded 300 tweets\n",
      "\tDownloaded 400 tweets\n",
      "\tDownloaded 500 tweets\n",
      "\tDownloaded 600 tweets\n",
      "\tDownloaded 700 tweets\n",
      "\tDownloaded 800 tweets\n",
      "\tDownloaded 900 tweets\n",
      "\tDownloaded 1000 tweets\n",
      "\tDownloaded 1100 tweets\n",
      "\tDownloaded 1200 tweets\n",
      "\tDownloaded 1300 tweets\n",
      "\tDownloaded 1400 tweets\n",
      "\tDownloaded 1500 tweets\n",
      "\tDownloaded 1600 tweets\n",
      "\tDownloaded 1700 tweets\n",
      "\tDownloaded 1800 tweets\n",
      "\tDownloaded 1900 tweets\n",
      "\tDownloaded 2000 tweets\n",
      "\tDownloaded 2100 tweets\n",
      "\tDownloaded 2200 tweets\n",
      "\tDownloaded 2300 tweets\n",
      "\tDownloaded 2400 tweets\n",
      "\tDownloaded 2500 tweets\n",
      "\tDownloaded 2600 tweets\n",
      "\tDownloaded 2700 tweets\n",
      "\tDownloaded 2800 tweets\n",
      "\tDownloaded 2900 tweets\n",
      "\tDownloaded 3000 tweets\n",
      "\tDownloaded 3100 tweets\n",
      "\tDownloaded 3200 tweets\n",
      "\tDownloaded 3300 tweets\n",
      "\tDownloaded 3400 tweets\n",
      "\tDownloaded 3500 tweets\n",
      "\tDownloaded 3600 tweets\n",
      "\tDownloaded 3700 tweets\n",
      "\tDownloaded 3800 tweets\n",
      "\tDownloaded 3900 tweets\n",
      "\tDownloaded 4000 tweets\n",
      "\tDownloaded 4100 tweets\n",
      "\tDownloaded 4200 tweets\n",
      "\tDownloaded 4300 tweets\n",
      "\tDownloaded 4400 tweets\n",
      "\tDownloaded 4500 tweets\n",
      "\tDownloaded 4600 tweets\n",
      "\tDownloaded 4700 tweets\n",
      "\tDownloaded 4800 tweets\n",
      "\tDownloaded 4900 tweets\n",
      "\tDownloaded 5000 tweets\n",
      "Finished!\n",
      "\n",
      "tweets_df4 Shape - (2147, 1)\n"
     ]
    }
   ],
   "source": [
    "tweets_df4 = twitter_client.get_tweets(keywords[3], max_tweets = 5000)\n",
    "print(f'tweets_df4 Shape - {tweets_df4.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching tweets for #KnowThe2020Candidates...\n",
      "No more tweets found.\n",
      "Finished!\n",
      "\n",
      "tweets_df5 Shape - (0, 0)\n"
     ]
    }
   ],
   "source": [
    "tweets_df5 = twitter_client.get_tweets(keywords[4], max_tweets = 5000)\n",
    "print(f'tweets_df5 Shape - {tweets_df5.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching tweets for #POTUS2020...\n",
      "\tDownloaded 100 tweets\n",
      "\tDownloaded 200 tweets\n",
      "\tDownloaded 300 tweets\n",
      "\tDownloaded 363 tweets\n",
      "No more tweets found.\n",
      "Finished!\n",
      "\n",
      "tweets_df6 Shape - (149, 1)\n"
     ]
    }
   ],
   "source": [
    "tweets_df6 = twitter_client.get_tweets(keywords[5], max_tweets = 5000)\n",
    "print(f'tweets_df6 Shape - {tweets_df6.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching tweets for #2020America...\n",
      "No more tweets found.\n",
      "Finished!\n",
      "\n",
      "tweets_df7 Shape - (0, 0)\n"
     ]
    }
   ],
   "source": [
    "tweets_df7 = twitter_client.get_tweets(keywords[6], max_tweets = 5000)\n",
    "print(f'tweets_df7 Shape - {tweets_df7.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame Shape - (5032, 1)\n"
     ]
    }
   ],
   "source": [
    "# Merge together all of the DataFrames\n",
    "# Note that tweets_df5 and tweets_df7 are both empty.\n",
    "\n",
    "final_df = pd.concat([tweets_df1, tweets_df2, tweets_df3, tweets_df4, tweets_df5, tweets_df6, tweets_df7])\n",
    "print(f'Final DataFrame Shape - {final_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a .csv file\n",
    "final_df.to_csv('tweets2020.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
