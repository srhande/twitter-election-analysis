{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we even begin to classify sentiments of sentences?\n",
    "Our project requires us to analyze the sentiments of tweets, which (obviously) consist of sentences and words. Sentences, of course, can come in numerous variations, and can also be considered unstructured due to this inconsistency between sentences. \n",
    "\n",
    "However, machine learning techniques that we use structured data in order to properly train models. So, how do we do train models on sentences?\n",
    "\n",
    "In the domain of Natural Language Processing (NLP), there are several techniques of **feature extraction**, or ways in which to structure text. As stated in [this paper on text classification algorithms](https://arxiv.org/pdf/1904.08067.pdf), feature extraction techniques serves to convert unstructured text sequences into a structured feature space once the textual data itself has been cleaned.\n",
    "\n",
    "For the purposes of this project, we will be looking at three of such techniques: **Bag-of-Words**, **Term Frequency-Inverse Document Frequency**, and **Word2Vec**.\n",
    "\n",
    "## How do we go about cleaning textual data?\n",
    "There are many ways in which to clean textual data, many of which are mentioned in the aforementioned paper. Some of these include tokenization, which breaks sentences into meaningful chunks (e.g. words or symbols) that are called tokens.\n",
    "Others include removing whitespace and special characters, converting all letters to lower-case, or removing stop words i.e. words that do not contain important significance such as *\"a\"* or *\"the\"*.\n",
    "\n",
    "## What is Bag-of-Words?\n",
    "The Bag-of-Words (BoW) model seeks to reduce and simplify text based on a specific criteria, most commonly word frequency. Think of a body of text as a literal bag of words (or list of words), where our feature space becomes the frequency that each word occurs in the text. The logic behind this is that words are often representative of the content of the sentence, so if a particular noun appears many times, then one can assume that the subject of that sentence has to do with that noun.\n",
    "\n",
    "[Wikipedia provides an excellent example of how this would exactly look like, and how this would work in spam filtering](https://en.wikipedia.org/wiki/Bag-of-words_model).\n",
    "\n",
    "However, there are some limitations of this approach. BoW ignores grammar and order of appearance of words e.g. \"Is this true\" and \"This is true\" both have the same feature space. There are also issues of scalability. However, since tweets have a character limit of 280 characters, we do not think this will be much of a problem.\n",
    "\n",
    "## What is Term Frequency-Inverse Document Frequency?\n",
    "Term Frequency-Inverse Document Frequency, otherwise known as TF-IDF, is another method of feature extraction, and is used to determine *how* important a word is to a document. How does it determine the importance of a word? It helps to explain this by first defining what these two terms mean.\n",
    "\n",
    "Term Frequency refers to the raw count of a term in a document. It is similar to how the Bag-of-Words model works, and just looks at the true frequency of some specific term.\n",
    "\n",
    "Whereas Term Frequency deals with the raw count of a term, Inverse Document Frequency refers to giving more *weight* to the uncommon terms, while giving less *weight* to more common terms This is calculated as an inverse function of the number of documents the term appears in.\n",
    "\n",
    "[Again, Wikipedia provides a good example of how this works out in finding documents relating to a specific query](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Term_frequency_2).\n",
    "\n",
    "Though TF-IDF gives weight to less common words, which helps lessen the effect common words have, this technique still has its cons. It cannot account for the similarity between words in a document, as each word is independently presented as an index. Also, since it is based off of the BoW Model, it does not capture positions in text or the semantics of the sentence.\n",
    "\n",
    "## What is Word2Vec?\n",
    "Word2Vec, short for *word to vector*, is an improvement on existing word embedding techniques. What is [word embedding](https://en.wikipedia.org/wiki/Word_embedding)? In short, these refer to NLP techniques that mapping words or phrases to vectors of real numbers i.e. a mapping of a space with many dimensions to a continuous vector space with much lower dimensions.\n",
    "\n",
    "What makes Word2Vec stand out? Word2Vec uses shallow neural networks with just 2 hidden layers trained to reconstruct the linguistic contexts of words. These models take as input a large corpus of words and creates a vector space of (usually) several hundred dimensions, where each unique word is given a vector in that vector space. Word vectors sharing a common context are closer to each other in that vector space than unrelated words e.g. \"small\" and \"smaller\" are closer than \"small\" and \"sky\".\n",
    "\n",
    "This method provides a means to discover the relationship and similarities between words, which is unattainable for the previous two methods described. \n",
    "\n",
    "[Word2Vec is described more in detail here, as well as how it can utilize Continuous ]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
